{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"view-in-github\",\n",
    "        \"colab_type\": \"text\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"<a href=\\\"https://colab.research.google.com/github/tolluset/hh-ai-1/blob/w3/%08week3/3-basic.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# [3주차 기본과제] DistilBERT로 뉴스 기사 분류 모델 학습하기\\n\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"sbgz49PvHhLt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 89,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"1LqgujQUbv6X\",\n",
    "        \"outputId\": \"f8c15a7a-da76-4eb5-cecd-eca412ed759a\",\n",
    "        \"collapsed\": true\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        \n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 90,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"6lGiZUoPby6e\",\n",
    "        \"outputId\": \"7c7c010f-3741-441d-b2e9-a67f3ae1e2ae\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"import torch\\n\",\n",
    "        \"from datasets import load_dataset\\n\",\n",
    "        \"from torch.utils.data import DataLoader\\n\",\n",
    "        \"\\n\",\n",
    "        \"tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## ✅  AG_News dataset 준비\\n\",\n",
    "        \"\\n\",\n",
    "        \"- ✅ Huggingface dataset의 `fancyzhx/ag_news`를 load합니다.\\n\",\n",
    "        \"- ✅ `collate_fn` 함수에 다음 수정사항들을 반영하면 됩니다.\\n\",\n",
    "        \"    - ✅ Truncation과 관련된 부분들을 지웁니다.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"r1oVPoix29Yj\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"ds = load_dataset(\\\"fancyzhx/ag_news\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"def collate_fn(batch):\\n\",\n",
    "        \"  max_len = 400\\n\",\n",
    "        \"  texts, labels = [], []\\n\",\n",
    "        \"  for row in batch:\\n\",\n",
    "        \"    labels.append(row['label'])\\n\",\n",
    "        \"    texts.append(row['text'])\\n\",\n",
    "        \"\\n\",\n",
    "        \"  encoding = tokenizer(texts, padding=True, return_tensors='pt')\\n\",\n",
    "        \"\\n\",\n",
    "        \"  texts, attention_mask = encoding['input_ids'], encoding['attention_mask']\\n\",\n",
    "        \"  labels = torch.tensor(labels)\\n\",\n",
    "        \"\\n\",\n",
    "        \"  return texts, attention_mask, labels\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_loader = DataLoader(\\n\",\n",
    "        \"    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\\n\",\n",
    "        \")\\n\",\n",
    "        \"test_loader = DataLoader(\\n\",\n",
    "        \"    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\\n\",\n",
    "        \")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"rE-y8sY9HuwP\"\n",
    "      },\n",
    "      \"execution_count\": 91,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 92,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"HJaUp2Vob0U-\",\n",
    "        \"outputId\": \"84c7ec28-9be2-4447-92b4-d558a9cb5a35\",\n",
    "        \"collapsed\": true\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"execute_result\",\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"DistilBertModel(\\n\",\n",
    "              \"  (embeddings): Embeddings(\\n\",\n",
    "              \"    (word_embeddings): Embedding(30522, 768, padding_idx=0)\\n\",\n",
    "              \"    (position_embeddings): Embedding(512, 768)\\n\",\n",
    "              \"    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n\",\n",
    "              \"    (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"  )\\n\",\n",
    "              \"  (transformer): Transformer(\\n\",\n",
    "              \"    (layer): ModuleList(\\n\",\n",
    "              \"      (0-5): 6 x TransformerBlock(\\n\",\n",
    "              \"        (attention): MultiHeadSelfAttention(\\n\",\n",
    "              \"          (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"          (q_lin): Linear(in_features=768, out_features=768, bias=True)\\n\",\n",
    "              \"          (k_lin): Linear(in_features=768, out_features=768, bias=True)\\n\",\n",
    "              \"          (v_lin): Linear(in_features=768, out_features=768, bias=True)\\n\",\n",
    "              \"          (out_lin): Linear(in_features=768, out_features=768, bias=True)\\n\",\n",
    "              \"        )\\n\",\n",
    "              \"        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n\",\n",
    "              \"        (ffn): FFN(\\n\",\n",
    "              \"          (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"          (lin1): Linear(in_features=768, out_features=3072, bias=True)\\n\",\n",
    "              \"          (lin2): Linear(in_features=3072, out_features=768, bias=True)\\n\",\n",
    "              \"          (activation): GELUActivation()\\n\",\n",
    "              \"        )\\n\",\n",
    "              \"        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n\",\n",
    "              \"      )\\n\",\n",
    "              \"    )\\n\",\n",
    "              \"  )\\n\",\n",
    "              \")\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"execution_count\": 92\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\\n\",\n",
    "        \"model\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# ✅ Classifier output, loss function, accuracy function 변경\\n\",\n",
    "        \"- ✅  뉴스 기사 분류 문제는 binary classification이 아닌 일반적인 classification 문제입니다. MNIST 과제에서 했던 것 처럼 `nn.CrossEntropyLoss` 를 추가하고 `TextClassifier`의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정하시면 됩니다.\\n\",\n",
    "        \"- 그리고 정확도를 재는 `accuracy` 함수도 classification에 맞춰 수정하시면 됩니다.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"UIcTzq726b14\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 93,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"xW7ETZQzzNp2\",\n",
    "        \"outputId\": \"9a2b1fca-77ab-4f53-e089-6866fe507478\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from torch import nn\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class TextClassifier(nn.Module):\\n\",\n",
    "        \"  def __init__(self):\\n\",\n",
    "        \"    super().__init__()\\n\",\n",
    "        \"\\n\",\n",
    "        \"    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\\n\",\n",
    "        \"    self.classifier = nn.Linear(768, 4)\\n\",\n",
    "        \"\\n\",\n",
    "        \"  def forward(self, input_ids, attention_mask):\\n\",\n",
    "        \"    x = self.encoder(input_ids, attention_mask)['last_hidden_state']\\n\",\n",
    "        \"    x = self.classifier(x[:, 0])\\n\",\n",
    "        \"\\n\",\n",
    "        \"    return x\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"model = TextClassifier()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 94,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"uyTciaPZ0KYo\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"for param in model.encoder.parameters():\\n\",\n",
    "        \"  param.requires_grad = False\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# ✅ 학습 결과 report\\n\",\n",
    "        \"- ✅  DistilBERT 실습과 같이 매 epoch 마다의 train loss를 출력하고 최종 모델의 test accuracy를 report합니다.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"TPH7Fa9n_p6M\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 95,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"XvvaAEwCznt-\",\n",
    "        \"outputId\": \"19ace1f8-69ad-4648-da39-50ae68986ed3\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Epoch   0 | Train Loss: 73.79941156506538\\n\",\n",
    "            \"Epoch   1 | Train Loss: 44.467991292476654\\n\",\n",
    "            \"Epoch   2 | Train Loss: 37.692022517323494\\n\",\n",
    "            \"Epoch   3 | Train Loss: 36.21875162422657\\n\",\n",
    "            \"Epoch   4 | Train Loss: 35.33278389275074\\n\",\n",
    "            \"Epoch   5 | Train Loss: 33.67221947014332\\n\",\n",
    "            \"Epoch   6 | Train Loss: 33.545470505952835\\n\",\n",
    "            \"Epoch   7 | Train Loss: 34.18277934193611\\n\",\n",
    "            \"Epoch   8 | Train Loss: 31.80599258840084\\n\",\n",
    "            \"Epoch   9 | Train Loss: 32.547775372862816\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from torch.optim import Adam\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"lr = 0.001\\n\",\n",
    "        \"model = model.to('cuda')\\n\",\n",
    "        \"loss_fn = nn.CrossEntropyLoss()\\n\",\n",
    "        \"\\n\",\n",
    "        \"optimizer = Adam(model.parameters(), lr=lr)\\n\",\n",
    "        \"n_epochs = 10\\n\",\n",
    "        \"\\n\",\n",
    "        \"for epoch in range(n_epochs):\\n\",\n",
    "        \"  total_loss = 0.\\n\",\n",
    "        \"  model.train()\\n\",\n",
    "        \"  for data in itertools.islice(train_loader, 100):\\n\",\n",
    "        \"    model.zero_grad()\\n\",\n",
    "        \"    inputs, attention_mask, labels = data\\n\",\n",
    "        \"    inputs, attention_mask, labels = inputs.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\\n\",\n",
    "        \"\\n\",\n",
    "        \"    preds = model(inputs, attention_mask)\\n\",\n",
    "        \"    loss = loss_fn(preds, labels)\\n\",\n",
    "        \"    loss.backward()\\n\",\n",
    "        \"    optimizer.step()\\n\",\n",
    "        \"\\n\",\n",
    "        \"    total_loss += loss.item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"  print(f\\\"Epoch {epoch:3d} | Train Loss: {total_loss}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 96,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"DjphVwXL00E2\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"outputId\": \"3d9bff1f-7b26-4796-80af-26d8dfaec442\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"=========> Train acc: 0.897 | Test acc: 0.890\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"import itertools\\n\",\n",
    "        \"\\n\",\n",
    "        \"def accuracy(model, dataloader):\\n\",\n",
    "        \"  cnt = 0\\n\",\n",
    "        \"  acc = 0\\n\",\n",
    "        \"\\n\",\n",
    "        \"  for data in itertools.islice(dataloader, 100):\\n\",\n",
    "        \"    inputs, attention_mask, labels = data\\n\",\n",
    "        \"    inputs, attention_mask, labels = inputs.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\\n\",\n",
    "        \"\\n\",\n",
    "        \"    preds = model(inputs, attention_mask)\\n\",\n",
    "        \"    preds = torch.argmax(preds, dim=-1)\\n\",\n",
    "        \"    # preds = (preds > 0).long()[..., 0]\\n\",\n",
    "        \"\\n\",\n",
    "        \"    cnt += labels.shape[0]\\n\",\n",
    "        \"    acc += (labels == preds).sum().item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"  return acc / cnt\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"with torch.no_grad():\\n\",\n",
    "        \"  model.eval()\\n\",\n",
    "        \"  train_acc = accuracy(model, train_loader)\\n\",\n",
    "        \"  test_acc = accuracy(model, test_loader)\\n\",\n",
    "        \"  print(f\\\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\\\")\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"colab\": {\n",
    "      \"gpuType\": \"T4\",\n",
    "      \"provenance\": [],\n",
    "      \"include_colab_link\": true\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
